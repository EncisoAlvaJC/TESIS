%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Conceptos, matem\'aticas}

En esta secci\'on se describen los conceptos b\'asicos de la teor\'ia espectral 'cl\'asica' para 
procesos estacionarios y no-estacionarios. 
De forma m\'as bien pragm\'atica, la descripci\'on est\'a
 fuertemente inspirada por el libro 'Spectral Analysis and Time Series' 
de M. Priestley \cite{Priestley81}, ya que este est\'a expl\'icitamente dirigido a un p\'ublico 
sin un trasfondo matem\'atico.

Esta redacci\'on supone conocidos los temas b\'asicos de probabilidad y estad\'istica:
variables aleatorias, valores esperados y momentos, estimadores y sus propiedades.
Con el fin de presentar la notaci\'on usada, se incluyen algunos conceptos previos a la 
definici\'on per se de estacionariedad y estimadores en el dominio de frecuencias.
%Chatfield (The Analysis of Time Series: An Introduction, 2003)

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Estacionariedad d\'ebil}

Para hablar formalmente de procesos estoc\'asticos como modelos, antes 
conviene escribir su definici\'on desde el punto de vista matem\'atico. Las siguientes definiciones
son aplicables tanto para procesos en tiempo continuo
como para procesos a tiempo discreto; aunque el objeto de estudio, el EEG, se considera 
un fen\'omeno continuo, s\'olo es posible registrarlo durante un conjunto finito de puntos 
en el tiempo.

\begin{defn}[Proceso estoc\'astico]
Un proceso estoc\'astico $\{ X(t) \}$ es una familia de variables aleatorias 
en los reales
indexadas por el 
$t$ que pertenece a alg\'un conjunto $T \in \R$.
%, mientras que una observaci\'on
%de $\{X(t)\}$ ser\'a denotada por $(x_1,x_2,\dots)$
\end{defn}

Como notaci\'on, una realizaci\'on de $X(t)$ ser\'a denota por $x_t$. 
Las funciones de densidad de probabilidad y de probabilidad acumulada para $X(t)$ ser\'an
referidas, respectivamente, como $f_{X(t)}$ y $F_{X(t)}$
%Cabe destacar que en esta definici\'on se omiti\'o intencionalmente pedir que las variables
%aleatorias sea reales, ya que eventualmente se considerar\'an procesos en los complejos.
Cabe enfatizar que para cada valor de $t$,
%tiempo $t$, 
$X(t)$ es una variable aleatoria; no se presupone ninguna conexi\'on entre ellas.
%con su funci\'on de densidad de probabilidad,
%sus momentos [s\'olo se consideran va's con al menos segundos momentos finitos], etc.


La caracter\'istica principal 
investigada en este trabajo hace referencia a la ''estacionariedad''. De manera 
informal, esta propiedad se refiere a que las variables aleatorias que conforman un proceso
estoc\'astico no cambien en el tiempo. 
Una definici\'on que satisface fielmente est\'a descripci\'on es la de estacionariedad 
en el sentido fuerte o estricto.
El t\'ermino ''tiempos admisibles'' simplemente indica que la definici\'on es la misma para
procesos a tiempo discreto o continuo, bajo restricciones obvias.

\begin{defn}[Estacionariedad fuerte]
Un proceso estoc\'astico $\{ X(t) \}$ es fuertemente estacionario si, para cualquier 
conjunto de tiempos admisibles $t_1,t_2,\dots,t_n$ y cualquier $\tau$ tal que 
 $t_i+\tau$ son tiempos admisibles para $i = 1, 2, \dots n$;
se cumple que
\begin{equation*}
F_{\left(X(t_1),X(t_2),\dots,X(t_n)\right) }
\equiv
F_{\left(X(t_1+\tau),X(t_2+\tau),\dots,X(t_n+\tau)\right)}
\end{equation*}

Donde $F_{\left(X(t_1),X(t_2),\dots,X(t_n)\right) }$ es la funci\'on de distribuci\'on de
probabilidad conjunta de $\left(X(t_1),X(t_2),\dots,X(t_n)\right)$
\end{defn}

Esta definici\'on, sin embargo, no resulta muy \'util en el contexto de la estad\'istica:
si se supone que el registro de un fen\'omeno puede interpretarse como \textbf{una} 
realizaci\'on de
un proceso estoc\'astico, entonces para cada tiempo se tiene una \'unica observaci\'on
de cada variable aleatoria. A esto hay que a\~nadir que, para un fen\'omeno continuo,
no todas los tiempos son registrables.
Luego, si no existe la garant\'ia de que las propiedades de estas variables aletorias sean
''similares'', entonces es virtualmente imposible obtener mayor informaci\'on de ellas.

Es bajo estas limitaciones que se motiva un concepto de estacionariedad m\'as d\'ebil, pero que
satisfaga ''suficientes teoremas importantes'' y que sea relevante bajo las restricciones
propias de diferentes campos. En este trabajo se ha optado por la llamada 
''estacionariedad d\'ebil'' o estacionariedad de orden 2, que recibe su nombre como caso
particular de la ''estacionariedad de orden $m$''.

\begin{defn}[Estacionariedad de orden $m$]
Un proceso estoc\'astico $\{ X(t) \}$
se dice estacionario de orden $m$ si, para cualquier 
conjunto de tiempos admisibles $t_1,t_2,\dots,t_n$ y cualquier $\tau \in \R$
se cumple que
\begin{equation*}
\E{ X^{m_1}(t_1)X^{m_2}(t_2)\cdots X^{m_n}(t_n) }
=
\E{ X^{m_1}(t_1+\tau)X^{m_2}(t_2+\tau)\cdots X^{m_n}(t_n+\tau) }
\end{equation*}
Para cualesquiera enteros $m_1,m_2,\dots,m_n$ tales que $m_1+m_2+\dots+m_n \leq m$
\end{defn}

La estacionariedad d\'ebil no pide que la funci\'on
de distribuci\'on conjunta tenga determinada forma, sino que los momentos conjuntos sean 
invariantes ante traslaciones en el tiempo. Para entender mejor esta diferencia, consid\'erense
tres procesos $\{X(t)\}$, $\{Y_1(t)\}$ y $\{Y_2(t)\}$, de modo que el primero es
estacionario en el sentido fuerte, el segundo es estacionario de orden 1 y el tercero es
estacionario de orden 2.
\begin{itemize}
\item Como por definici\'on $F_{X(t) } \equiv F_{X(t+\tau)}$ para cualesquieras $t$, $t+\tau$
admisibles, entonces claramente $\E{X(t)} = \mu_X$ es una constante
\item De igual manera, por definici\'on,
para cualesquieras $t$, $t+\tau$ admisibles se tiene que $\E{Y_1(t)}=\E{Y_1(t+\tau)}$ y
$\E{Y_2(t)}=\E{Y_2(t+\tau)}$. Se deduce entonces que $\E{Y_1(t)} = \mu_{Y_1}$, 
$\E{Y_2(t)} = \mu_{Y_2}$ son constantes
\item Usando nuevamente que $F_{X(t) } \equiv F_{X(t+\tau)}$ para cualesquieras $t$, $t+\tau$
admisibles, entonces se deduce que $\Var{X(t)} = \sigma_X$ es una constante
\item Para $i = 1,2$ se puede notar que
$$\Var{Y_i(t)} = \E{Y_i^{2}(t)} - \left( \E{Y_i(t)} \right)^{2} = \E{Y_i^{2}(t)} - \mu_{Y_i}$$
luego, por la sola definici\'on de $Y_1$ y $Y_2$, se puede deducir que $\E{Y_2^{2}(t)}$ es
constante en el tiempo mientras que no ocurre lo mismo para $Y_1$. Se concluye que la varianza
de $Y_2$ es constante en el tiempo mientras que la de $Y_1$ no necesariamente lo es
\end{itemize}

Naturalmente hay una relaci\'on de contenci\'on clara en
la familia de los conjuntos de procesos estacionarios hasta un orden finito:
si un proceso es estacionario de orden $m$, entonces es estacionario de orden $n$ para todo
$n \leq m$. Es posible incluso describir procesos que sean estacionarios de orden ''infinito''
y preguntarse bajo qu\'e condiciones son fuertemente estacionarios.
Tal discusi\'on no se incluye en el presente trabajo con el fin de ser concreto.

%(...)

Dadas las consideraciones anteriores, conviente
introducir una segunda caracter\'izaci\'on de los procesos estacionarios de orden 2 --d\'ebilmente 
estacionarios--
 enlistar las siguientes propiedades para un proceso
estacionario de orden 2 --o d\'ebilmente estacionario-- 
que es equivalente
%\footnote{Este hecho se vuelve claro si se analizan cuidadosamente las propiedades
%enlistadas y se comparan con las deficiniones de las ''cantidades'' involucradas}
a la definici\'on anterior pero cuya interpretaci\'on suele considerarse como m\'as clara
%Hay una especie de consenso seg\'un el cual la estacionariedad de orden 2, tambi\'en
%llamada \textbf{estacionariedad d\'ebil} es suficiente para
%que se cumplan los teoremas m\'as comunes sobre medias y varianzas.
%Algunas consecuencias que un
%proceso sea estacionario debilmente son las siguientes:
\begin{thrm}
Un proceso es d\'ebilmente estacionario si y s\'olo si para cualesquiera tiempos admisibles
$t$, $s$ se tiene que
\begin{itemize}
\item $\E{X(t)} = \mu_X$
\item $\Var{X(t)} = \sigma^{2}_X$
\item $\Cov{X(t),X(s)} = \rho (s-t)$
\end{itemize}
Donde $\mu_X$, $\sigma^{2}_X$ son constantes, $\rho(\tau)$
es una funci\'on que \'unicamente depende de $\tau$
\end{thrm}

%El rec\'iproco tambi\'en es cierto: si un proceso cumple las tres condiciones anteriores,
%entonces es estacionario de orden 2. A su vez tres condiciones son m\'as usuales en la literatura
%y tienen una intepretaci\'on m\'as clara como modelo, pues se exige que el proceso tenga media
%y varianza constante, y que la funci\'on de autocorrelaci\'on no dependa de d\'onde se mida --lo
%cual simplifica la estimaci\'on de estas cantidades.

Cabe mencionar como comentario que es de hecho posible contruir procesos que sean fuertemente
estacionarios peor que no sean estacionarios de ning\'un orden finito; dado que
la primera definici\'on se basa en funciones de densidad de probabilidad, mientras que la segunda
se basa en momentos, es suficiente con considerar variables aleatorias que no tengan todos
sus momentos bien definidos. Por ejemplo, consid\'erese un proceso conformado por variables
aleatorias independientes id\'enticamente distribuidas con distribuci\'on de Cauchy.

Dado que en el EEG se miden fluctuaciones en potenciales de campos el\'ectricos
%(medida en mV),
que (en este trabajo) se modelan como variables aleatorias, 
la intepretaci\'on usual para los momentos de estas variables est\'a ligado a la distribuci\'on de
energ\'ia asociada al sistema. Luego, es plausible considerar que el EEG es un fen\'omeno
''suficientemente regular'' como para que las variables aleatorias del modelo tengan cuando
menos segundos momentos bien definidos.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Espectro de un proceso estacionario}

Existe una larga tradici\'on en las ciencias biom\'edicas para interpretar a los registros
electrofisiol\'ogicos en t\'erminos de ondas y frecuencias, ya que fundamentalmente se
trata de fen\'omenos el\'ectricos \cite{Kaiser00}. 
As\'imismo existe una teor\'ia matem\'atica bien desarrollada sobre estad\'istica en el 
llamado ''dominio de las frecuencias''. 
En este trabajo se aborda la segunda como forma de tener coherencia con la primera; a continuaci\'on
se describen los conceptos m\'as importantes en el modelo usado.

Un objeto fundamental para el estudio del dominio de las frecuencias\footnote{Este concepto no se
discutir\'a en el presente trabajo, sino que ser\'a manejado pragm\'aticamente para referirse
al cambio de coordenadas inducido por la transformada de Fourier o alguna generalizaci\'on de la 
misma} son las series de Fourier. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

[[aqui van las definiciones de transformada e integrales de fourier y fourier stiltjes]]

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Una pregunta natural cuando se toma la terminolog\'ia de ondas y frecuencias dentro
del estudio de series de tiempo, es sobre el significado de aplicar la transformada de Fourier a
un proceso estoc\'atico --o cuando menos a alguna sus realizaciones.
Â¿Bajo qu\'e condiciones las realizaciones de un proceso estoc\'astico admiten una representaci\'on
como series/integrales de Fourier/Fourier-Stieltjes?

(Por simplicidad se abordar\'a primero esta pregunta para procesos a tiempo continuo, y 
posteriormente se tratar\'a el caso a tiempo disreto.)

Se sabe que una condici\'on suficiente para que exista la transformada de Fourier de una funci\'on
dada, es que pertenezca al espacio de las funciones $\L2$, definido como
\begin{equation*}
L^2 = \left\{ f: \R \rightarrow \R {\biggr\rvert} \int_{-\infty}^{\infty} f(x) dx < \infty \right\}
\end{equation*}

Sin embargo, considerando un proceso estacionario $\{ X(t) \}$, y dado que tiene varianza constante 
en el tiempo, se 
espera que sus realizaciones $x(t)$ no decaigan en infinito. Por otro lado, tampoco hay garant\'ia
que admita una representaci\'on de Fourier-Stieltjes. M\'as a\'un, no hay garant\'ia alguna que
una realizaci\'on arbitraria pueda expresarse como la suma de una funci\'on en $L^2$ y una
funci\'on que admita representaci\'on de Fourier-Stieltjes.

El enfoque que se aborda es construir una sucesi\'on de funciones 
%peri\'odicas 
en $L^{2}$
que convergen a ''cada''
$x(t)$, y luego revisar la convergencia de sus respectivas integrales de Fourier.
As\'i entonces, para cada $T>0$ se define
\begin{equation}
x_T(t) = 
\begin{cases}
x(t) & \text{ , } -T\leq t \leq T
\\
0 & \text{ , otro caso}
\end{cases}
\end{equation}

Claramente, para todo $T$ se tiene que $x_T \in L^2$, y entonces admite la siguiente 
representaci\'on
\begin{equation}
x_T (t) = \frac{1}{\sqrt{2 \pi}} \intR G_T(\omega) e^{i \omega t} d\omega
\end{equation}

Donde se define la funci\'on $G_T$ como

\begin{equation}
G_T (\omega) = \frac{1}{\sqrt{2 \pi}} \intR x_T(t) e^{-i \omega t} dt
= \frac{1}{\sqrt{2 \pi}} \int_{-T}^{T} x(t) e^{-i \omega t} dt
\end{equation}

Como se mencion\'o anteriormente no hay garant\'ia de que $x(t)$, una realizaci\'on arbitraria de
$\{X(t)\}$, tenga una integral de Fourier bien definida. Luego entonces no hay garant\'ia que 
$G_T$ converja cuando $T\rightarrow \infty$. Recuperando la interpretaci\'on de 
$\left| G_T(\omega) \right|^{2}$ como una funci\'on de densidad para la energ\'ia total del sistema 
asociada a la frecuencia
puntual $\omega$, destaca un argumento f\'isico seg\'un el cual $G_T$ no tiene por qu\'e converger:
durante un tiempo infinito, un sistema que maneja ''niveles constantes'' de 
energ\'ia puede registrar una cantidad infinita de energ\'ia en su historial. 
%Curiosamente,
%esta segunda interpretaci\'on del
%problema motiva una soluci\'on para el mismo: usando un tipo de promedio para la
%distribuci\'on de energ\'ia involucrando a $T$
Este problema puede remediarse resolviendo el enredo de palabras y t\'erminos, 
ya que no es tan importante la
cantidad de energ\'ia concentrada en cada frecuencia, sino qu\'e frecuencias concentran m\'as
energ\'ia. Luego entonces conviene usar un promedio que ''tome en cuenta'' el tama\~no
del intervalo

\begin{equation}
\lim_{T\rightarrow{\infty}} = \frac{ \left| G_T(\omega) \right|^{2}}{2 T}
\label{yacasi}
\end{equation}

La expresi\'on en \ref{yacasi} es una adaptaci\'on de la integral de Fourier para una realizaci\'on
de un proceso estoc\'astico a tiempo continuo; los detalles sobre la convergencia de esta 
cantidad se discutir\'an m\'as adelante.
%conserva una parte clave de la interpretaci\'on.
Por mientras, en cierta medida se ha contestado una de las interrogantes al inicio de esta 
secci\'on sobre la posibilidad y
el posible significado de una transformada de Fourier para las realizaciones de un proceso 
estoc\'astico; con respecto a la posibilidad de una transformada para el proceso per se, vale la 
pena
ajustar la definici\'on en \ref{yacasi} para que sea ''representativa'' del proceso --y no s\'olo
de una realizaci\'on particular. Priestley introduce la siguiente funci\'on

\begin{equation}
h(\omega) = \lim_{T\rightarrow \infty} \E{ \frac{ \left| G_T(\omega) \right|^{2}}{2 T} }
\end{equation}

La funci\'on $h$ es referida como la \textbf{funci\'on de densidad espectral no-normalizada} para
$X(t)$. Conviene, adem\'as, definir una \textbf{funci\'on de espectro integrado}


-------------------------------------------------------------------------------

\subsection{Teorema de Wiener-Khintchine}

\begin{thrm}[Wiener-Khintchine]
Una condici\'on suficiente y necesaria para que $\rho$ sea una funci\'on de autocorrelaci\'on de 
alg\'un proceso estoc\'astico a tiempo continuo $\{X(t)\}$ estacionario y estoc\'asticamente 
continuo, es que exista una funci\'on $F$ que tenga las 
%mismas propiedades que una funci\'on de 
%densidad de probabilidad
siguientes propiedades
\begin{itemize}
\item Monotonamente creciente
\item $F(-\infty) = 0$
\item $F(\infty) = 1$
\end{itemize}
y tal que para todo $\tau \in \R$ se cumple que
\begin{equation*}
\rho(\tau) = \intR e^{i \omega \tau} dF(\omega)
\end{equation*}
\end{thrm}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\subsubsection{Continuidad estoc\'astica}
%
%Aunque la demonstraci\'on del teorema de Khinchin-Kolmogorov

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Quiero y me siento obligado a citar la excelente discuci\'on
filos\'ofica
de Loynes \cite{Loynes68}, resaltando la frase ''Los espectros instant\'aneos no existen''.
Tambi\'en quiero citar una discusi\'on m\'as moderna de M\'elard \cite{Melard89}, donde una
frase a favor es ''El supuesto de estacionariedad ha sido v\'alido previamente debido a la corta
duraci\'on de las series y la baja capacidad de c\'omputo''.

Pues la mayor parte de mi trabajo se ha centrado en el concepto de \textbf{espectro} de una serie
de tiempo. La mejor forma de introducir el espectro evolutivo 
--en el sentido que estoy usando-- es
presentar un proceso estacionario de orden 2,
 $\{X(t)\}$, en su representaci\'on de Cram\'er \cite{Priestley81}
[la existencia de esta representacion esta garantizada por el teorema de Khinchin-Wiener --para
procesos a tiempo continuos-- y por una extension del mismo por Wold --para procesos a tiempo
discreto.
por ahora solo cito el resultado, pero quiza sea buena idea escribir la demostracion
como apendice, una demostracion citada ya que es bastante tecnica]

\begin{equation*}
X(t) = \int_{\Lambda} A(\omega) e^{i 2\pi \omega t} dZ(\omega)
\end{equation*}

Donde el proceso $\{ Z(\omega) \}$ tiene incrementos ortogonales, es decir 
\begin{equation*}
\Cov{dZ(\omega_1,dZ(\omega_2))} = \delta(\omega_1,\omega_1) d\omega
\end{equation*}
Con $\delta$ la funci\'on delta de Dirac. Cabe mencionar que es suficiente si los incrementos
son independientes, pero se puede debilitar ese requerimiento; incluso es de notarse que no
se exige que el proceso sea al menos continuo --en el sentido estoc\'astico.

El espectro de potencia de $\{X(t)\}$ se define como

\begin{equation*}
f(\omega) = \abso{A(\omega)}^{2}
\end{equation*}

Citar\'e de Adak \cite{Adak98} una tabla donde compara varias definiciones de espectro, para
procesos no-estacionarios.

\begin{figure}[h]
\centering
\includegraphics[width=0.9\textwidth]{tabla.png} 
\end{figure}

Dos identidades muy importantes para estimar el espectro son la \textit{equivalencia} entre
el espectro y la funci\'on de autocorrelaci\'on

\begin{equation*}
f(\omega ) = \int R_X(\tau ) e^{-i 2\pi \omega t} d\tau
\end{equation*}

Donde funci\'on de autocorrelaci\'on se ha definido como

\begin{equation*}
R_X(\tau) = E\left[ X(t) X(t+\tau) \right] = \int_0^{\infty} X(t)X(t+\tau) dt
\end{equation*}

[la demostracion es corta, batsa con reescribir una composicion de integrales como convolucion,
la incluire mas tarde]

Por otro lado, se tiene la Identidad de Parseval

\begin{equation*}
\int X^{2}(t) dt = \int f(\omega) d\omega
\end{equation*}

[esta demostracion se basa en la convergencia dominada del modulo de la integral de $X^{2}$ por
la integral del modulo (...), la incluire mas tarde]

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Test Priestley-Subba Rao (PSR)}

(seccion en proceso de re-redaccion)

A muy grosso modo, el test PSR estima localmente  el espectro evolutivo
 y revisa si estad\'isticamente
cambia en el tiempo.

Para ello, usa un estimador para la funci\'on de densidad espectral
que es aproximadamente (asint\'oticamente) insesgado y cuya varianza est\'a
determinada aproximadamente. La estimaci\'on se lleva a cabo en puntos en el tiempo y
la frecuencia tales que en conjunto son aproximadamente no-correlacionados.
Se aplica logaritmo para que la varianza de todos los estimadores sea aproximadamente
la misma (el logaritmo ayuda a), amen que los errores conjuntos tengan una
distribuci\'on cercana a una multinormal con correlaci\'on cero.
Finalmente se aplica una prueba ANOVA de varianza conocida.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{El espectro evolutivo}

Consid\'erese un proceso estoc\'astico a tiempo continuo $\{X(t)\}$, tal que
$E[X(t)]=0$ y $E\left[ X^{2}(t)\right] < \infty$ para todo $t$. Es decir que su media es constante
y sus segundos momentos est\'an bien definidos, aunque 
estos \'ultimos pueden cambiar con el tiempo.

Por el momento se supondr\'a que acepta una representaci\'on de la forma

\begin{equation*}
X(t) = \int_{-\pi}^{\pi} A(t ; \omega) e^{i\omega t} \, d Z(\omega)
\end{equation*}

Con $\{ Z(\omega) \}$ una familia de procesos ortogonales\footnote{De nuevo, esto implica que
$\Cov{dZ(\omega_1,dZ(\omega_2))} = \delta(\omega_1,\omega_1) d\omega$, una condici\'on m\'as
d\'ebil que la independencia} tales que

\begin{itemize}
\item $E \left[\abso{ dZ(\omega)}^{2} \right] = d\omega$
\item Para cada $t$ el m\'aximo de $A(t;\cdot)$ se encuentra en 0
\end{itemize}

Esta representaci\'on es an\'aloga a la representaci\'on de Cram\'er para un proceso
estacionario, salvo que se permite que la funci\'on $A$ cambie con el tiempo.
Siguiendo la analog\'ia, se define 
el \textbf{espectro evolutivo} de $\{X(t)\}$, con respecto a la la familia
$\mathcal{F} = \{ e^{i\omega t} A(t; \omega) \}$
 como
 
\begin{equation*}
d F(\omega;t) = \lvert A(t;\omega) \lvert^{2} d\omega
\end{equation*}

Ahora bien, si se supone que $\{X(t)\}$ es estoc\'asticametne diferenciable, entonces
se puede definir una \textbf{funci\'on de densidad espectral}

\begin{equation*}
f(t;\omega) = \lvert A(t;\omega) \lvert^{2}
\end{equation*}

Cabe destaca que si la funci\'on $A(t;\omega)$ fuera constante con respecto a $t$, se obtendr\'ia
un proceso estacionario de orden dos tal cual fue descrito en la secci\'on anterior.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{El estimador de doble ventana}

Esta t\'ecnica fue presentada por Priestley en 1965. Muy a grosso modo, es un estimador de la
funci\'on de densidad espectral con ciertas propiedades y que parte de la idea que un proceso
no-estacionario puede verse localmente como un proceso lineal generalizado.

Como meta-nota, yo empec\'e a estudiar este tipo de estimadores porque es \textit{el qeu ven\'ia
con el m\'etodo} ya que el test esta implementado en R; desde un punto de vista de difusi\'on,
es una ventaja usar un m\'etodo implementado en un software gratuito y de c\'odigo abierto --y
no una mera excusa para no explorar otros m\'etodos. En todo caso, he revisado varios otros test,
pero de momento solo este ha arrojado suficientes resultados para llenar un informe.

%{Estimador de doble ventana (Priestley, 1965 \& 1966)}
Para construir el estimador se reuieren dos funciones, $g$ y $w_T$, que servir\'an como ventanas
para extraer informaci\'on local de los datos. Debido a que sus propiedades tienen una interpretaci\'on
f\'isica desde la teor\'ia de circuitos, absorben su terminolog\'ia

\textit{
nota al pie: deberia incluir una motivacion de estos nombres,
que en parte tiene relevancia en la interpretacion. Los 
Linear Invariant Systems (LIS) suponen dependencia lineal
--constante-- respecto a todos los tiempos anteriores; 
a tiempo continuo son equivalentes a una ecuacion diferencial ordinaria lineal,
y a su vez a modelos AR. Un modelo fisico para ello son los circuitos RC, que
fueron usables en radios, y para los cuales las palabras 'filtro' y 'frecuencia'
tienen una interpretacion clara. Esta terminologia de circuitos electricos tiene sentido
para mi ya que todos los modelos de neuronas y poblaciones de neuronas que he visto hasta ahora,
por ejemplo de Ermentrout (falta citar), {Clark98,Priestley81}, PARTEN de considerar
circuitos equivalentes a los componentes neuronales, lo cual me hace pensar que es buena idea
mantener esta vision conjunta.
}

Primeramente se toma una funci\'on $g(u)$ normalizada, que en conjunto a su
transformada inversa de Fourier\footnote{Esta funci\'on 
$\Gamma(u) = \int_{-\infty}^{\infty} g(u) e^{i u \omega} du$
es referida como
\textbf{frequency-response function}, nombre tiene un poco de encanto cuando
$g$ adopta ciertas formas particulares (senos y cosenos).} 
$\Gamma$ tiene las siguientes propiedades

\begin{equation*}
2\pi \int_{-\infty}^{\infty} \lvert g(u) \lvert^{2} du 
= 
\int_{-\infty}^{\infty} \lvert \Gamma(\omega) \lvert^{2} d\omega
= 1
\end{equation*}


A partir de $g$ y $\Gamma$ se define el filtro $U$ como una convoluci\'on
con las realizaciones del proceso

\begin{equation*}
U(t,\omega) = \int_{t-T}^{t} g(u) X({t-u}) e^{i \omega (t-u)} du
\end{equation*}

Un ejemplo que est\'a en el libro de Priestley es tomar funciones del tipo

\begin{equation*}
g_h(u) = 
\begin{cases}
{1 \big{/} 2\sqrt{\pi h}} & \text{ , } \abso{u} \leq h
\\
0 & \text{ , } \abso{u} > 0
\end{cases}
\end{equation*}

Su correspondiente funci\'on de respuesta de frecuencia es complicada [me falta 
escribirla]. Es referida como la \textbf{ventana de Bartlett} y
est\'a totalmente caracterizada la siguiente propiedad

\begin{equation*}
\abso{\Gamma_h(\omega)}^{2} = \frac{1}{\pi h} \left( \frac{\text{sen} (h \omega)}{\omega} \right)^{2}
\end{equation*}

Cabe mencionar que puede entenderse al par $g$ y $\Gamma$ como ventanas en el tiempo
y las frecuencias para la serie.

---

Ahora bien, se toma una segunda ventana $W_\tau$ con las siguientes
restricciones para
su funci\'on de respuesta ante frecuencia $w_\tau$

\begin{itemize}
\item $w_{\tau}(t) \geq 0$ para cualesquiera $t$, $\tau$
\item $w_{\tau}(t) \rightarrow 0$ cuando $\lvert t \lvert \rightarrow \infty$, para todo $\tau$
\item $\displaystyle \int_{-\infty}^{\infty} w_{\tau}(t) dt = 1$ para todo $\tau$
\item $\displaystyle \int_{-\infty}^{\infty} \left( w_{\tau}(t) \right)^{2} dt < \infty$ para todo $\tau$
\item Existe una constante $C$ tal que  [T est\'a relacionado con el 'tiempo 0', pero para
tiempos de muestreo grandes se puede reemplazar por $-\infty$ EXCEPTO cerca del inicio y el final dle muestreo]
$$\lim_{\tau\rightarrow\infty} \left[ \tau \int_{t-T}^{t} \lvert W_{\tau}(\lambda) \lvert^{2} d\lambda \right] = C$$
\end{itemize}

%Ahora, si se define 
%$\displaystyle W_{T'}(\lambda) = \int_{-\infty}^{\infty} e^{-i\lambda t}w_{T'}(t) dt $

[posteriormente annadire mas detalles sobre el papel que juega el par $w_\tau$, $W_\tau$]

Como ejemplo, se puede tomar la siguiente funci\'on llamada \textbf{ventana de Daniell}

\begin{equation*}
W_\tau (t) = 
\begin{cases}
{1 \big{/} \tau} & \text{ , } -\nicefrac{1}{2} \tau \leq t \leq \nicefrac{1}{2} \tau
\\
0 & \text{ , otro caso}
\end{cases}
\end{equation*}

La cual se puede demostrar [tengo en algun lado esa demostracion]

$$\lim_{\tau\rightarrow\infty} \left[ \tau \int_{t-T}^{t} \lvert W_{\tau}(\lambda) \lvert^{2} d\lambda \right] = 2\pi$$

-----

Se define el estimador para $f_t$, con $0 \leq t \leq T$
\begin{equation*}
\widehat{f_t}(\omega) = \int_{t-T}^{t} w_{T'}(u) \lvert U(t-u,\omega) \lvert^{2} du
\end{equation*}

Fue demostrado por Priestley (1965, falta citar) que 

[aqui van las expresiones para el valor esperado y la varianza de $\widehat{f_t}$, me falta
escribirlas]

Pero, bajo varios supuesto adicionales [que me falta trascribir] se puede aproximar

\begin{equation*}
E\left[ \widehat{f_t}(\omega) \right] \sim f_t(\omega)
\end{equation*}

\begin{equation*}
\Var{\widehat{f_t}(\omega)} 
\sim 
\frac{C}{\tau} \left(f_t(\omega)\right)^{2} \int_{-\infty}^{\infty} \abso{\Gamma(\theta)}^{4} d\theta
\end{equation*}

Se advierte claramente que $\widehat{f_t}$ es unnestimados aproximadamente insesgado.
Para las ventanas de Bartlett y Daniell usadas como ejemplo, se tiene

\begin{equation*}
\Var{\widehat{f_t}} 
\sim 
\frac{4h}{3\tau} \left(f_t(\omega)\right)^{2}
\end{equation*}

Cabe mencionar que hay una expresi\'on expl\'icita para la covarianza de $\widehat{f_t}$
en para diferentes puntos en el tiempo y las frecuencias. Lamentablemente,
aun me falta escribirlas, son complicadas, y se describen situaciones bajo las
cuales estas covarianzas son negligibles; cabe destacar que TODAS las condiciones 
que se usan para aproximar son b\'asicamente las mismas, y dependen de que la distancia
entre los tiempos y las frecuencias sean tan grandes como sea posible.

------------

El \'ultimo ingrediente del test PSR es una transformaci\'on logar\'itmica
para regular la varianza, y quiza para cortar los bordes de las aproxiamciones.
Se define $Y_{i,j} = \log \left( \widehat{f_{t_i}}(\omega_j) \right)$, con las siguientes propiedades

\begin{equation*}
E\left[ Y_{i,j} \right] \thicksim \log \left( f_{t_i}(\omega_j) \right)
\hspace{4em}
\text{Var}\left( {Y\left(t,\omega\right)}\right) \thicksim \sigma^{2}
\end{equation*}

Luego as\'i, puede escribirse aproximadamente que

$$Y_{i,j} = \log \left( f_{t_i}(\omega_j) \right) + \varepsilon_{i,j}$$

donde $\varepsilon_{i,j}$ va iid tales que

$
E\left[ \varepsilon_{i,j} \right] = 0
\hspace{4em}
\text{Var}\left( \varepsilon_{i,j} \right) \sigma^{2}
$

Priestley cita que con esta informaci\'on incluso se puede considerar que los $\varepsilon_{i,j}$
siguen una distribuci\'on normal cada uno; Nason (2015, falta citar) comenta que
este supuesto no tiene por que cumplirse, y que es una popsible fuente de falsos positivos
para el test. Yo he hecho pruebas de normalidad a los datos, que incluire como anexos
mas tarde.

El test PSR \textit{per se} son tres test ANOVA --en su versi\'on en la que la varianza es conocida--
sobre si los $\varepsilon_{i,j}$ son estad\'isticamente negligibles en total, sobre el tiempo y sobre
las frecuencias. Para el fin de estudiar la estacionariedad, basta con que sean estad\'iticamente
no-negligibles sobre el tiempo.

[Por supuesto que los otros dos test tienen interpretacion: la negigibilidad total da informacion
sobre las marginales, y si estas pueden ser estimadas adecuadamente usando el estimador, si se
combina con negativo para no-estacionariedad es \textbf{efectivamente positivo} para estacionariedad
y toma una forma muy particular (proceso uniformemente modulado). Si sobre las frecuencias resulta
significativo (no-negligible) da informacion sobre la 'aeatoridad total' del proceso.
De tener tiempo, lo incluire como anexo, ya que ninguna de estas caracteristicas es estudiada :( ]

Lo detalles de la implementaci\'on en R estar\'an en la secci\'on de resultados.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%