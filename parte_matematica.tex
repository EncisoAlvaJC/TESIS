%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Conceptos, matem\'aticas}

En esta secci\'on se describen los conceptos b\'asicos de la teor\'ia espectral 'cl\'asica' para 
procesos estacionarios, y la generalizaci\'on hecha por Priestley para procesos no-estacionarios. 
De forma m\'as bien pragm\'atica, la descripci\'on est\'a
 fuertemente inspirada por el libro 'Spectral Analysis and Time Series' 
de M. B. Priestley \cite{Priestley81}, ya que este est\'a expl\'icitamente dirigido a un p\'ublico 
sin un trasfondo matem\'atico.

Se suponen conocidos varios temas b\'asicos de probabilidad y estad\'istica:
variables aleatorias, valores esperados y momentos, estimadores y sus propiedades.
Con el fin de presentar la notaci\'on usada, se incluyen algunos conceptos previos a la 
definici\'on per se de estacionariedad y estimadores en el dominio de frecuencias.
%Chatfield (The Analysis of Time Series: An Introduction, 2003)

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Estacionariedad d\'ebil}

Para hablar formalmente de procesos estoc\'asticos como modelos, antes 
conviene escribir su definici\'on desde el punto de vista matem\'atico. Las siguientes definiciones
son aplicables tanto para procesos en tiempo continuo
como para procesos a tiempo discreto; aunque el objeto de estudio, el EEG, se considera 
un fen\'omeno continuo, s\'olo es posible registrarlo durante un conjunto finito de puntos 
en el tiempo.

\begin{defn}[Proceso estoc\'astico]
Un proceso estoc\'astico $\{ X(t) \}$ es una familia de variables aleatorias 
en los reales,
indexadas por
$t \in T \subseteq \R$.
%, mientras que una observaci\'on
%de $\{X(t)\}$ ser\'a denotada por $(x_1,x_2,\dots)$
\end{defn}

Como notaci\'on, una realizaci\'on de $X(t)$ ser\'a denota por $x_t$. 
Las funciones de densidad de probabilidad y de probabilidad acumulada para $X(t)$ ser\'an
referidas, respectivamente, como $f_{X(t)}$ y $F_{X(t)}$
%Cabe destacar que en esta definici\'on se omiti\'o intencionalmente pedir que las variables
%aleatorias sea reales, ya que eventualmente se considerar\'an procesos en los complejos.
Cabe enfatizar que para cada valor de $t$,
%tiempo $t$, 
$X(t)$ es una variable aleatoria; no se presupone ninguna conexi\'on entre ellas.
%con su funci\'on de densidad de probabilidad,
%sus momentos [s\'olo se consideran va's con al menos segundos momentos finitos], etc.


La caracter\'istica principal 
investigada en este trabajo hace referencia a la ''estacionariedad''. De manera 
informal, esta propiedad se refiere a que las variables aleatorias que conforman un proceso
estoc\'astico sean b\'asicamente iguales --dicho con otras palabras, que las propiedades
del proceso sean invariantes en el tiempo. 
Una definici\'on que satisface fielmente est\'a descripci\'on es la de estacionariedad 
en el sentido fuerte o estricto.
El t\'ermino ''tiempos admisibles'' simplemente indica que la definici\'on es la misma para
procesos a tiempo discreto o continuo, bajo restricciones obvias.

\begin{defn}[Estacionariedad fuerte]
Un proceso estoc\'astico $\{ X(t) \}$ es fuertemente estacionario si, para cualquier 
conjunto de tiempos admisibles $t_1,t_2,\dots,t_n$ y cualquier $\tau$ tal que 
 $t_i+\tau$ son tiempos admisibles para $i = 1, 2, \dots n$;
se cumple que
\begin{equation*}
F_{\left(X(t_1),X(t_2),\dots,X(t_n)\right) }
\equiv
F_{\left(X(t_1+\tau),X(t_2+\tau),\dots,X(t_n+\tau)\right)}
\end{equation*}

Donde $F_{\left(X(t_1),X(t_2),\dots,X(t_n)\right) }$ es la funci\'on de distribuci\'on de
probabilidad conjunta de $\left(X(t_1),X(t_2),\dots,X(t_n)\right)$
\end{defn}

Esta definici\'on, sin embargo, no resulta muy \'util en el contexto de la estad\'istica:
si se supone que el registro de un fen\'omeno puede interpretarse como \textbf{una} 
realizaci\'on de
un proceso estoc\'astico, entonces para cada tiempo se tiene una \'unica observaci\'on
de cada variable aleatoria. A esto hay que a\~nadir que, para un fen\'omeno continuo,
no todas los tiempos son registrables.
Luego, si no existe la garant\'ia de que las propiedades de estas variables aletorias sean
''similares'', entonces es virtualmente imposible obtener mayor informaci\'on de ellas.

Es bajo estas limitaciones que se motiva un concepto de estacionariedad m\'as d\'ebil, pero que
satisfaga ''suficientes teoremas importantes'' y que sea relevante bajo las restricciones
propias de diferentes campos. En este trabajo se ha optado por la llamada 
''estacionariedad d\'ebil'' o estacionariedad de orden 2, que recibe su nombre como caso
particular de la ''estacionariedad de orden $m$''.

\begin{defn}[Estacionariedad de orden $m$]
Un proceso estoc\'astico $\{ X(t) \}$
se dice estacionario de orden $m$ si, para cualquier 
conjunto de tiempos admisibles $t_1,t_2,\dots,t_n$ y cualquier $\tau \in \R$
se cumple que
\begin{equation*}
\E{ X^{m_1}(t_1)X^{m_2}(t_2)\cdots X^{m_n}(t_n) }
=
\E{ X^{m_1}(t_1+\tau)X^{m_2}(t_2+\tau)\cdots X^{m_n}(t_n+\tau) }
\end{equation*}
Para cualesquiera enteros $m_1,m_2,\dots,m_n$ tales que $m_1+m_2+\dots+m_n \leq m$
\label{est_orden_m}
\end{defn}

La estacionariedad d\'ebil no pide que la funci\'on
de distribuci\'on conjunta tenga determinada forma, sino que los momentos conjuntos sean 
invariantes ante traslaciones en el tiempo. Para entender mejor esta diferencia, consid\'erense
tres procesos $\{X(t)\}$, $\{Y_1(t)\}$ y $\{Y_2(t)\}$, de modo que el primero es
estacionario en el sentido fuerte, el segundo es estacionario de orden 1 y el tercero es
estacionario de orden 2.
\begin{itemize}
\item Por definici\'on $F_{X(t) } \equiv F_{X(t+\tau)}$ para cualesquieras $t$, $t+\tau$
admisibles; entonces $\E{X(t)} = \mu_X$ es constante
\item Por definici\'on
para cualesquieras $t$, $t+\tau$ admisibles se tiene que $\E{Y_1(t)}=\E{Y_1(t+\tau)}$ y
$\E{Y_2(t)}=\E{Y_2(t+\tau)}$. Se deduce que $\E{Y_1(t)} = \mu_{Y_1}$, 
$\E{Y_2(t)} = \mu_{Y_2}$ son constantes
\item Usando nuevamente que $F_{X(t) } \equiv F_{X(t+\tau)}$ para cualesquieras $t$, $t+\tau$
admisibles, se deduce que $\Var{X(t)} = \sigma_X$ es constante
\item Por definici\'on de $\mathrm{Var}$ y de $Y_i$ ($i=2,1$)
$$\Var{Y_i(t)} = \E{Y_i^{2}(t)} - \left( \E{Y_i(t)} \right)^{2} = \E{Y_i^{2}(t)} - \mu_{Y_i}$$
Luego se puede deducir que 
$\Var{Y_2(t)}$ es
% $\E{Y_2^{2}(t)}$ es
constante, mientras que no se puede garantizar lo mismo para $\Var{Y_1(t)}$
% Se concluye que la varianza
%de $Y_2$ es constante en el tiempo mientras que la de $Y_1$ no necesariamente lo es
\item El \textit{coeficiente de asimetr\'ia de Fisher} 
de una variable aleatoria $V$ se define como
$$
\gamma_1(V) = \frac{\E{\left(V-\E{V}\right)^{3}}}{\Var{V}^{\nicefrac{3}{2}}}
$$
%= \frac{\E{X^{3}}-3\mu_V \E{V^{2}}+2\mu_V^{3}}{\Var{V}^{\nicefrac{3}{2}}}
%$$
Sin entrar en detalles, se puede deducir que $\gamma_1(X(t))$ es constante mientras que no se
puede garantizar lo mismo para $\gamma_1(Y_1(t))$, $\gamma_1(Y_2(t))$
\end{itemize}

Naturalmente hay una relaci\'on de contenci\'on clara en
la familia de los conjuntos de procesos estacionarios de orden finito:
si un proceso es estacionario de orden $m$, entonces es estacionario de orden $n$ para todo
$n \leq m$. Es posible incluso describir procesos que sean estacionarios de orden ''infinito''
y preguntarse bajo qu\'e condiciones son fuertemente estacionarios.
Tal discusi\'on no se incluye en el presente trabajo.
%,
%% con el fin de ser concreto.
%ya que 
%%el enfoque es de modelaci\'on y 
%se ha intentado hacer la menor cantidad posible de supuestos y de proporcionara a cada uno una
%''interpretaci\'on fisiol\'ogica''.


%As\'i pues, y dadas 
Una vez hechas
las consideraciones anteriores, conviente
introducir una segunda caracter\'izaci\'on de los procesos estacionarios de orden 2 --d\'ebilmente 
estacionarios--
% enlistar las siguientes propiedades para un proceso
%estacionario de orden 2 --o d\'ebilmente estacionario-- 
que es equivalente
%\footnote{Este hecho se vuelve claro si se analizan cuidadosamente las propiedades
%enlistadas y se comparan con las deficiniones de las ''cantidades'' involucradas}
a la definici\'on \ref{est_orden_m} pero cuya interpretaci\'on suele considerarse como m\'as clara
%Hay una especie de consenso seg\'un el cual la estacionariedad de orden 2, tambi\'en
%llamada \textbf{estacionariedad d\'ebil} es suficiente para
%que se cumplan los teoremas m\'as comunes sobre medias y varianzas.
%Algunas consecuencias que un
%proceso sea estacionario debilmente son las siguientes:
\begin{thrm}
Un proceso es d\'ebilmente estacionario si y s\'olo si para cualesquiera tiempos admisibles
$t$, $s$ se tiene que
\begin{itemize}
\item $\E{X(t)} = \mu_X$
\item $\Var{X(t)} = \sigma^{2}_X$
\item $\Cov{X(t),X(s)} = \rho (s-t)$
\end{itemize}
Donde $\mu_X$, $\sigma^{2}_X$ son constantes, $\rho(\tau)$
es una funci\'on que \'unicamente depende de $\tau$
\label{est_usual}
\end{thrm}

A grosso modo, cuando uno se refiere a un proceso d\'ebilmente estacionario seg\'un
\ref{est_orden_m} se le pide que su primer y segundo momentos sean constantes, as\'i como
el primer momento conjunto s\'olo dependa del lag en el tiempo. A su vez, seg\'un 
\ref{est_usual} un proceso es d\'ebilmente estacionario si su media y varianza son constantes en
el tiempo, y su funci\'on de autocovarianza s\'olo depende del lag en el tiempo.

Cabe mencionar, como comentario, que es posible contruir procesos que sean fuertemente
estacionarios pero que no sean estacionarios de ning\'un orden finito; dado que
la primera definici\'on se basa en funciones de densidad de probabilidad mientras que la segunda
se basa en momentos, es suficiente con usar variables aleatorias que no tengan todos
sus momentos bien definidos. Por ejemplo, consid\'erese un proceso conformado por variables
aleatorias independientes id\'enticamente distribuidas con distribuci\'on de Cauchy.

Dado que en el EEG se miden fluctuaciones en potenciales de campos el\'ectricos
%(medida en mV),
que (en este trabajo) se modelan como variables aleatorias, 
la intepretaci\'on usual para los momentos de estas variables est\'a ligado a la distribuci\'on de
energ\'ia asociada al sistema. Luego, es plausible considerar que el EEG es un fen\'omeno
''suficientemente regular'' como para que las variables aleatorias del modelo tengan cuando
menos segundos momentos bien definidos.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Espectro de un proceso estacionario}

Existe una larga tradici\'on en las ciencias biom\'edicas para interpretar a los registros
electrofisiol\'ogicos en t\'erminos de ondas y frecuencias, ya que fundamentalmente se
trata de fen\'omenos el\'ectricos \cite{Kaiser00}. 
As\'imismo existe una teor\'ia matem\'atica bien desarrollada sobre estad\'istica en el 
llamado ''dominio de las frecuencias''. 
En este trabajo se aborda la segunda como forma de tener coherencia con la primera; a continuaci\'on
se describen los conceptos m\'as importantes en el modelo usado.

Un objeto fundamental para el estudio del dominio de las frecuencias\footnote{Este concepto 
no se
%discutir\'a en el presente trabajo, sino que 
ser\'a manejado pragm\'aticamente para referirse
al cambio de coordenadas inducido por la transformada de Fourier o alguna generalizaci\'on de la 
misma} son las series de Fourier y sus generalizaciones. 

\begin{defn}[Serie de Fourier (funciones peri\'odicas)]
Sea $f$ una funci\'on peri\'odica con periodo $2\pi$ tal que $\intR \abso{f(t)} dt < \infty$. 
Si se calculan los coeficientes
\begin{equation*}
A_n = \frac{1}{2\pi} \intPI f(t) e^{- i n t} dt
\end{equation*}
entonces la siguiente igualdad se cumple casi en todas partes
\begin{equation*}
f(x) = \sum_{n=-\infty}^{\infty} A_n e^{i n t}
\end{equation*}
La sucesi\'on $\left( A_n \right)$ ser\'a referida como \textbf{serie de Fourier} de la funci\'on
$f$.
\label{FourierClasico}
\end{defn}

%\begin{defn}[Transformada de Fourier (funciones peri\'odicas)]
%Sea $f$ una funci\'on peri\'odica con periodo $2\pi$ tal que $\intR \abso{f(t)} dt < \infty$.
%hjkjkjkj
%\end{defn}

Por el momento no se discutir\'an los detalles sobre la convergencia de las sucesiones de 
\ref{FourierClasico}, siempre que se limite a funciones continuas absolutamente sumables 
$L^{1}\left([-\pi,\pi])\right)$.
Asimismo, parece claro que se puede definir una funci\'on invertible que mapea las funciones que
poseen serie de Fourier a ''alg\'un'' conjunto de series absolutamente sumables $\ell$; esta
funci\'on es referida como \textbf{transformada de Fourier}, pero por el momento se considerar\'an
conocidas y demostradas todas sus propiedades.

%Las serie de Fourier ''cl\'asica'' se define para funciones peri\'odicas en el inervalo
%$[-\pi,\pi]$ tales que $\intR $

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Las series de Fourier gozan de una interpretación física muy extendida como que una se\~nal 
peri\'odica
puede verse como la superposici\'on de se\~nales peri\'odicas m\'as simples. 
De igual forma es destacable su interpretaci\'on como ''coordenadas'' en un espacio de funciones
dada una base ortonormal del mismo. 
El estudio de estos espacios dentro del an\'alisis trae a la mente
la cuesti\'on de convergencia,
el problema del subespacio de
funciones medibles de medida cero, y la posibilidad de otras bases; estos fen\'omenos tienen a su 
vez una interpretaci\'on f\'isica como cambios s\'ubitos en la energ\'ia, el ruido y la 
tipificaci\'on de ondas ''simples'' --por ejemplo, las ondas cuadradas y triangulares son 
m\'as comunes en teor\'ia de circuitos.

Para limar estas ambig\:uedades, en este trabajo se considerar\'a la base de Fourier como la
''m\'as natural'' por su conexi\'on simple con las exponenciales complejas. El t\'ermino
''ruido'' ser\'a evitado en la medida de lo posible, ya que en la terminolog\'ia de se\~nales
puede encontrarse referido a registros con un comportamiento err\'atico y poco
predecible; dentro del contexto de electrofisiolog\'ia, este concepto bien
engloba tanto se\~nales que
se desea estudiar como se\~nales que se busca eliminar. 
%Para los procesos estoc\'asticos que son considerados en este trabajo, 
Conviene definir
un tipo de ''regularidad estoc\'astica'' que sirva para distinguir los patrones buscados
de errores de medici\'on.

\begin{defn}[Continuidad estoc\'astica (media cuadr\'atica)]
Un proceso estoc\'astico a tiempo continuo $\{ X(t) \}$ es estoc\'asticamente continuo
(en el sentido de media cuadr\'atica)
en un tiempo admisible $t_0$ si y s\'olo si
\begin{equation*}
\lim_{t \rightarrow t_0} \E{\left( X(t) - X(t_0) \right)^{2}} = 0
\end{equation*}
\label{cont_est}
\end{defn}

Una forma natural de pensar en la definici\'on \ref{cont_est} es esperar que en promedio
$\lim_{t \rightarrow t_0} \left( X(t) - X(t_0) \right)^{2} = 0$. 
No es la \'unica forma de presentar un l\'imite de variables aletorias, sino que se ha elegido
esta forma por algunas propiedades que ser\'an explotadas m\'as adelante. 
As\'imismo cabe destacar que un proceso estoc\'asticamente continuo no necesariamente produce 
realizaciones que son funciones continuas, aunque sus realizaciones deben ser continuas 
casi en todas partes\footnote{Una funci\'on es \textit{continua casi en todas partes} si
es continua en todo su dominio excepto por un conjunto de medida cero}.
Para explorar este concepto de manera concreta, consid\'erese un proceso de Wiener; como es 
un ejemplo ''r\'apido'', se definir\'a a apartir de sus propiedades:

\begin{defn}[Proceso de Wiener]
Un proceso estoc\'astico a tiempo continuo $\{ W(t) \}$ recibe el nombre de proceso
de Wiener si satisface que
\begin{itemize}
\item $W(0) = 0$
\item La variable aleatoria $W(t) - W(s)$ tiene una distribuci\'on normal con media 0 y 
varianza $\abso{t-s}$
\item Las variables aleatorias ${W(t)}$ y ${W(t) - W(s)}$ son independientes para
todos los tiempo permitidos $t$, $s$
\end{itemize} 
\end{defn}

Ahora bien, se considera un proceso de Wiener $\{W(t)\}$ con $t\geq 0$, y se verificar\'a su 
continuidad estoc\'astica para un punto arbitrario $t_0 > 0$. Por definici\'on, se tiene
que 
$$W(t) - W(t_0) \sim N(0,\abso{t-t_0}) \sim \sqrt{\abso{t-t_0}} N(0,1)$$
donde el s\'imbolo $\sim$ indica que dos variables tienen la misma funci\'on de densidad de 
probabilidad. Luego, como $ \left( W(t) - W(t_0) \right)^{2} \sim \chi^{2}(1) $, se cumple que
$$
\E{\left( W(t) - W(t_0) \right)^{2}}
$$
y luego entonces es claro que $\lim_{t \rightarrow t_0} \E{\left( X(t) - X(t_0) \right)^{2}} = 0$.
Esta aritm\'etica de variables aleatorias puede formalizarse, pero a lo largo de
este trabajo se supondr\'an conocidos los detalles respectivos.

De manera m\'as general, se formula el teorema

\begin{thrm}
Un proceso d\'ebilmente estacionario a tiempo continuo es estoc\'asticamente continuo si y s\'olo si
su funci\'on de autocorrelaci\'on es continua en 0
\end{thrm}
\begin{demostracion}
Sea $\{ X(t) \}$ un proceso d\'ebilmente estacionario, y sea $t_0$ un tiempo admisible arbitrario. 
Luego, para todo $t$ admisible se cumple que
\begin{align*}
\E{\left( X(t) - X(t_0) \right)^{2}}
&= \Var{X(t)} + \Var{X(t_0)} - 2 \Cov{X(t)}{X(t_0)}
\\
&= 2 \sigma_X^{2} \left( 1 - \rho(t-t_0) \right)
\end{align*}
donde $\rho$ es la funci\'on de autocorrelaci\'on. Luego es claro que
\begin{equation*}
\lim_{t\rightarrow t_0} \E{\left( X(t) - X(t_0) \right)^{2}} = 0  
{\Leftrightarrow} 
\lim_{t\rightarrow t_0} 2 \sigma_X^{2} \left( 1 - \rho(t-t_0) \right) = 0
\equiv
\lim_{\tau \rightarrow 0} \rho(\tau) = 1
\end{equation*}
Como siempre se cumple que $\rho(0)=1$, la condici\'on final se traduce en que $\rho$ sea continua 
en 0
\end{demostracion}

Con esta segunda caracterizaci\'on a la mano, es f\'acil afirmar que un proceso donde todas
conformado por variables aleatorias independientes no es estoc\'asticamente continuo,
ya que satisface que su funci\'on de autocorrelaci\'on vale 0 en todos los puntos
excepto en 0, donde vale 1.

Entonces, en este trabajo se supondr\'a que los registros de PSG corresponden a realizaciones
de procesos estoc\'asticamente continuos; se considera la posiblidad de que est\'en
''contaminados'' por ''ruidos'', entendidos como procesos independientes de los potenciales de 
campo 
en el cerebro, de amplitud negligible y que ''muy posiblmente'' son estoc\'asticamente discontinuos 
casi en todas partes.

---------------------------------------------------------------------------------------------------

Con respecto al concepto de energ\'ia, en este trabajo se partir\'a formalmente desde un resultado
usual y se tomar\'a como definici\'on: la energ\'ia disipada por la se\~nal $f$ en el intervalo
de tiempo $(t_1,t_2)$ est\'a dada por $\int_{t_1}^{t_2} \left(f\left(t\right)\right)^{2} dt$.
Ante este contexto toma gran importancia la identidad de Parseval.

---------------------------------------------------------------------------------------------------

%En el contexto de series electrofisiol\'ogicas, se mencion\'o que en el EEG se han descrito y
%patrones llamados ''ondas de sue\~no'' \footnote{Para m\'as informaci\'on ver
%las secciones anteriores}. Hist\'oricamente estas ondas fueron tipificados mediante el 
%uso de registros electroencefalogr\'aficos en papel, de modo que se define emp\'iricamente
%la ''frecuencia'' de una onda se sue\~no al contar el n\'umero de altibajos en una unidad
%de tiempo \cite{Klonowski09}. En un plano muy formal, no se espera que una onda  de sue\~no
%tenga una transformada de Fourier, al menos en el sentido cl\'asico; por otro lado, se espera que
%pueda formalizarse el concepto de una onda ''parecida'' a unacua frecuencia es tal.

En este caso, se presentar\'a la transformada de Fourier-Stieltjes. En primera instancia
%permite frecuencias puntuales que son inconmensurables con respecto al intervalo $[-\pi,'pi]$,
%de modo que 
acepta funciones no-peri\'odicas pero que pueden ser representadas como suma de funciones
peri\'odicas, la diferencia m\'as notable es que permite involucar funciones cuya frecuencia
es inconmensurable con respecto al intervalo $[-\pi,\pi]$ como por ejemplo la funci\'on
$$
f(x) = \COS{x} + \COS{x\sqrt{2}}
$$
no tiene una serie de Fourier, pero puede ser representada como una integral de Fourier-Stieltjes.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Una pregunta natural cuando se toma la terminolog\'ia de ondas y frecuencias dentro
del estudio de series de tiempo, es sobre el significado de aplicar la transformada de Fourier a
un proceso estoc\'atico --o cuando menos a alguna sus realizaciones.
¿Bajo qu\'e condiciones las realizaciones de un proceso estoc\'astico admiten una representaci\'on
como series/integrales de Fourier/Fourier-Stieltjes?

(Por simplicidad se abordar\'a primero esta pregunta para procesos a tiempo continuo, y 
posteriormente se tratar\'a el caso a tiempo disreto.)

Se sabe que una condici\'on suficiente para que exista la transformada de Fourier de una funci\'on
dada, es que pertenezca al espacio de las funciones $\L2$, definido como
\begin{equation*}
L^2(\R) = \left\{ f: \R \rightarrow \R {\biggr\rvert} \int_{-\infty}^{\infty} f(x) dx < \infty \right\}
\end{equation*}

Sin embargo, considerando un proceso estacionario $\{ X(t) \}$, y dado que tiene varianza constante 
en el tiempo, se 
espera que sus realizaciones $x(t)$ no decaigan en infinito. Por otro lado, tampoco hay garant\'ia
que admita una representaci\'on de Fourier-Stieltjes. M\'as a\'un, no hay garant\'ia alguna que
una realizaci\'on arbitraria pueda expresarse como la suma de una funci\'on en $L^2$ y una
funci\'on que admita representaci\'on de Fourier-Stieltjes.

El enfoque que se aborda es construir una sucesi\'on de funciones 
%peri\'odicas 
en $L^{2}$
que convergen a ''cada''
$x(t)$, y luego revisar la convergencia de sus respectivas integrales de Fourier.
As\'i entonces, para cada $T>0$ se define
\begin{equation}
x_T(t) = 
\begin{cases}
x(t) & \text{ , } -T\leq t \leq T
\\
0 & \text{ , otro caso}
\end{cases}
\end{equation}

Claramente, para todo $T$ se tiene que $x_T \in L^2$, y entonces admite la siguiente 
representaci\'on
\begin{equation}
x_T (t) = \frac{1}{\sqrt{2 \pi}} \intR G_T(\omega) e^{i \omega t} d\omega
\end{equation}

Donde se define la funci\'on $G_T$ como

\begin{equation}
G_T (\omega) = \frac{1}{\sqrt{2 \pi}} \intR x_T(t) e^{-i \omega t} dt
= \frac{1}{\sqrt{2 \pi}} \int_{-T}^{T} x(t) e^{-i \omega t} dt
\end{equation}

Como se mencion\'o anteriormente no hay garant\'ia de que $x(t)$, una realizaci\'on arbitraria de
$\{X(t)\}$, tenga una integral de Fourier bien definida. Luego entonces no hay garant\'ia que 
$G_T$ converja cuando $T\rightarrow \infty$. Recuperando la interpretaci\'on de 
$\left| G_T(\omega) \right|^{2}$ como una funci\'on de densidad para la energ\'ia total del sistema 
asociada a la frecuencia
puntual $\omega$, destaca un argumento f\'isico seg\'un el cual $G_T$ no tiene por qu\'e converger:
durante un tiempo infinito, un sistema que maneja ''niveles constantes'' de 
energ\'ia puede registrar una cantidad infinita de energ\'ia en su historial. 
%Curiosamente,
%esta segunda interpretaci\'on del
%problema motiva una soluci\'on para el mismo: usando un tipo de promedio para la
%distribuci\'on de energ\'ia involucrando a $T$
Este problema puede remediarse resolviendo el enredo de palabras y t\'erminos, 
ya que no es tan importante la
cantidad de energ\'ia concentrada en cada frecuencia, sino qu\'e frecuencias concentran m\'as
energ\'ia. Luego entonces conviene usar un promedio que ''tome en cuenta'' el tama\~no
del intervalo

\begin{equation}
\lim_{T\rightarrow{\infty}} = \frac{ \left| G_T(\omega) \right|^{2}}{2 T}
\label{yacasi}
\end{equation}

La expresi\'on en \ref{yacasi} es una adaptaci\'on de la integral de Fourier para una realizaci\'on
de un proceso estoc\'astico a tiempo continuo; los detalles sobre la convergencia de esta 
cantidad se discutir\'an m\'as adelante.
%conserva una parte clave de la interpretaci\'on.
Por mientras, en cierta medida se ha contestado una de las interrogantes al inicio de esta 
secci\'on sobre la posibilidad y
el posible significado de una transformada de Fourier para las realizaciones de un proceso 
estoc\'astico; con respecto a la posibilidad de una transformada para el proceso per se, vale la 
pena
ajustar la definici\'on en \ref{yacasi} para que sea ''representativa'' del proceso --y no s\'olo
de una realizaci\'on particular. Priestley introduce la siguiente funci\'on

\begin{equation}
h(\omega) = \lim_{T\rightarrow \infty} \E{ \frac{ \left| G_T(\omega) \right|^{2}}{2 T} }
\end{equation}

La funci\'on $h$ es referida como la \textbf{funci\'on de densidad espectral no-normalizada} para
$\{X(t)\}$. 
Se espera poder definir $h$ para funciones deterministas --no sólo para proceso estoc\'asticos--
en el caso que exista una ''componente determinista'' que resulte ser importante; por ejemplo, los
una presencia marcada de ondas cerebrales.

\subsection{Teorema de Wiener-Khintchine}

\begin{thrm}[Wiener-Khintchine]
Una condici\'on suficiente y necesaria para que $\rho$ sea una funci\'on de autocorrelaci\'on de 
alg\'un proceso estoc\'astico a tiempo continuo $\{X(t)\}$ estacionario y estoc\'asticamente 
continuo, es que exista una funci\'on $F$ que tenga las 
%mismas propiedades que una funci\'on de 
%densidad de probabilidad
siguientes propiedades
\begin{itemize}
\item Monotonamente creciente
\item $F(-\infty) = 0$
\item $F(\infty) = 1$
\end{itemize}
y tal que para todo $\tau \in \R$ se cumple que
\begin{equation*}
\rho(\tau) = \intR e^{i \omega \tau} dF(\omega)
\end{equation*}
\end{thrm}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\subsubsection{Continuidad estoc\'astica}
%
%Aunque la demonstraci\'on del teorema de Khinchin-Kolmogorov

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Quiero y me siento obligado a citar la excelente discuci\'on
filos\'ofica
de Loynes \cite{Loynes68}, resaltando la frase ''Los espectros instant\'aneos no existen''.
Tambi\'en quiero citar una discusi\'on m\'as moderna de M\'elard \cite{Melard89}, donde una
frase a favor es ''El supuesto de estacionariedad ha sido v\'alido previamente debido a la corta
duraci\'on de las series y la baja capacidad de c\'omputo''.

%Pues la mayor parte de mi trabajo se ha centrado en el concepto de \textbf{espectro} de una serie
%de tiempo. La mejor forma de introducir el espectro evolutivo 
%--en el sentido que estoy usando-- es
%presentar un proceso estacionario de orden 2,
% $\{X(t)\}$, en su representaci\'on de Cram\'er \cite{Priestley81}
%[la existencia de esta representacion esta garantizada por el teorema de Khinchin-Wiener --para
%procesos a tiempo continuos-- y por una extension del mismo por Wold --para procesos a tiempo
%discreto.
%por ahora solo cito el resultado, pero quiza sea buena idea escribir la demostracion
%como apendice, una demostracion citada ya que es bastante tecnica]

\begin{equation*}
X(t) = \int_{\Lambda} A(\omega) e^{i 2\pi \omega t} dZ(\omega)
\end{equation*}

Donde el proceso $\{ Z(\omega) \}$ tiene incrementos ortogonales, es decir 
\begin{equation*}
\Cov{dZ(\omega_1,dZ(\omega_2))} = \delta(\omega_1,\omega_1) d\omega
\end{equation*}
Con $\delta$ la funci\'on delta de Dirac. Cabe mencionar que es suficiente si los incrementos
son independientes, pero se puede debilitar ese requerimiento; incluso es de notarse que no
se exige que el proceso sea al menos continuo --en el sentido estoc\'astico.

El espectro de potencia de $\{X(t)\}$ se define como

\begin{equation*}
f(\omega) = \abso{A(\omega)}^{2}
\end{equation*}

Citar\'e de Adak \cite{Adak98} una tabla donde compara varias definiciones de espectro, para
procesos no-estacionarios.

\begin{figure}[h]
\centering
\includegraphics[width=0.9\textwidth]{tabla.png} 
\end{figure}

Dos identidades muy importantes para estimar el espectro son la \textit{equivalencia} entre
el espectro y la funci\'on de autocorrelaci\'on

\begin{equation*}
f(\omega ) = \int R_X(\tau ) e^{-i 2\pi \omega t} d\tau
\end{equation*}

Donde funci\'on de autocorrelaci\'on se ha definido como

\begin{equation*}
R_X(\tau) = E\left[ X(t) X(t+\tau) \right] = \int_0^{\infty} X(t)X(t+\tau) dt
\end{equation*}

[la demostracion es corta, batsa con reescribir una composicion de integrales como convolucion,
la incluire mas tarde]

Por otro lado, se tiene la Identidad de Parseval

\begin{equation*}
\int X^{2}(t) dt = \int f(\omega) d\omega
\end{equation*}

%[esta demostracion se basa en la convergencia dominada del modulo de la integral de $X^{2}$ por
%la integral del modulo (...), la incluire mas tarde]

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Test Priestley-Subba Rao (PSR)}

%(seccion en proceso de re-redaccion)

A muy grosso modo, el test PSR estima localmente  el espectro evolutivo
 y revisa si estad\'isticamente
cambia en el tiempo.

Para ello, usa un estimador para la funci\'on de densidad espectral
que es aproximadamente (asint\'oticamente) insesgado y cuya varianza est\'a
determinada aproximadamente. La estimaci\'on se lleva a cabo en puntos en el tiempo y
la frecuencia tales que en conjunto son aproximadamente no-correlacionados.
Se aplica logaritmo para que la varianza de todos los estimadores sea aproximadamente
la misma (el logaritmo ayuda a), amen que los errores conjuntos tengan una
distribuci\'on cercana a una multinormal con correlaci\'on cero.
Finalmente se aplica una prueba ANOVA de varianza conocida.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{El espectro evolutivo}

Consid\'erese un proceso estoc\'astico a tiempo continuo $\{X(t)\}$, tal que
$E[X(t)]=0$ y $E\left[ X^{2}(t)\right] < \infty$ para todo $t$. Es decir que su media es constante
y sus segundos momentos est\'an bien definidos, aunque 
estos \'ultimos pueden cambiar con el tiempo.

Por el momento se supondr\'a que acepta una representaci\'on de la forma

\begin{equation*}
X(t) = \int_{-\pi}^{\pi} A(t ; \omega) e^{i\omega t} \, d Z(\omega)
\end{equation*}

Con $\{ Z(\omega) \}$ una familia de procesos ortogonales\footnote{De nuevo, esto implica que
$\Cov{dZ(\omega_1,dZ(\omega_2))} = \delta(\omega_1,\omega_1) d\omega$, una condici\'on m\'as
d\'ebil que la independencia} tales que

\begin{itemize}
\item $E \left[\abso{ dZ(\omega)}^{2} \right] = d\omega$
\item Para cada $t$ el m\'aximo de $A(t;\cdot)$ se encuentra en 0
\end{itemize}

Esta representaci\'on es an\'aloga a la representaci\'on de Cram\'er para un proceso
estacionario, salvo que se permite que la funci\'on $A$ cambie con el tiempo.
Siguiendo la analog\'ia, se define 
el \textbf{espectro evolutivo} de $\{X(t)\}$, con respecto a la la familia
$\mathcal{F} = \{ e^{i\omega t} A(t; \omega) \}$
 como
 
\begin{equation*}
d F(\omega;t) = \lvert A(t;\omega) \lvert^{2} d\omega
\end{equation*}

Ahora bien, si se supone que $\{X(t)\}$ es estoc\'asticametne diferenciable, entonces
se puede definir una \textbf{funci\'on de densidad espectral}

\begin{equation*}
f(t;\omega) = \lvert A(t;\omega) \lvert^{2}
\end{equation*}

Cabe destaca que si la funci\'on $A(t;\omega)$ fuera constante con respecto a $t$, se obtendr\'ia
un proceso estacionario de orden dos tal cual fue descrito en la secci\'on anterior.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{El estimador de doble ventana}

Esta t\'ecnica fue presentada por Priestley en 1965. Muy a grosso modo, es un estimador de la
funci\'on de densidad espectral con ciertas propiedades y que parte de la idea que un proceso
no-estacionario puede verse localmente como un proceso lineal generalizado.

Como meta-nota, yo empec\'e a estudiar este tipo de estimadores porque es \textit{el qeu ven\'ia
con el m\'etodo} ya que el test esta implementado en R; desde un punto de vista de difusi\'on,
es una ventaja usar un m\'etodo implementado en un software gratuito y de c\'odigo abierto --y
no una mera excusa para no explorar otros m\'etodos. En todo caso, he revisado varios otros test,
pero de momento solo este ha arrojado suficientes resultados para llenar un informe.

%{Estimador de doble ventana (Priestley, 1965 \& 1966)}
Para construir el estimador se reuieren dos funciones, $g$ y $w_T$, que servir\'an como ventanas
para extraer informaci\'on local de los datos. Debido a que sus propiedades tienen una interpretaci\'on
f\'isica desde la teor\'ia de circuitos, absorben su terminolog\'ia

\textit{
nota al pie: deberia incluir una motivacion de estos nombres,
que en parte tiene relevancia en la interpretacion. Los 
Linear Invariant Systems (LIS) suponen dependencia lineal
--constante-- respecto a todos los tiempos anteriores; 
a tiempo continuo son equivalentes a una ecuacion diferencial ordinaria lineal,
y a su vez a modelos AR. Un modelo fisico para ello son los circuitos RC, que
fueron usables en radios, y para los cuales las palabras 'filtro' y 'frecuencia'
tienen una interpretacion clara. Esta terminologia de circuitos electricos tiene sentido
para mi ya que todos los modelos de neuronas y poblaciones de neuronas que he visto hasta ahora,
por ejemplo de Ermentrout (falta citar), {Clark98,Priestley81}, PARTEN de considerar
circuitos equivalentes a los componentes neuronales, lo cual me hace pensar que es buena idea
mantener esta vision conjunta.
}

Primeramente se toma una funci\'on $g(u)$ normalizada, que en conjunto a su
transformada inversa de Fourier\footnote{Esta funci\'on 
$\Gamma(u) = \int_{-\infty}^{\infty} g(u) e^{i u \omega} du$
es referida como
\textbf{frequency-response function}, nombre tiene un poco de encanto cuando
$g$ adopta ciertas formas particulares (senos y cosenos).} 
$\Gamma$ tiene las siguientes propiedades

\begin{equation*}
2\pi \int_{-\infty}^{\infty} \lvert g(u) \lvert^{2} du 
= 
\int_{-\infty}^{\infty} \lvert \Gamma(\omega) \lvert^{2} d\omega
= 1
\end{equation*}


A partir de $g$ y $\Gamma$ se define el filtro $U$ como una convoluci\'on
con las realizaciones del proceso

\begin{equation*}
U(t,\omega) = \int_{t-T}^{t} g(u) X({t-u}) e^{i \omega (t-u)} du
\end{equation*}

Un ejemplo que est\'a en el libro de Priestley es tomar funciones del tipo

\begin{equation*}
g_h(u) = 
\begin{cases}
{1 \big{/} 2\sqrt{\pi h}} & \text{ , } \abso{u} \leq h
\\
0 & \text{ , } \abso{u} > 0
\end{cases}
\end{equation*}

Su correspondiente funci\'on de respuesta de frecuencia es complicada [me falta 
escribirla]. Es referida como la \textbf{ventana de Bartlett} y
est\'a totalmente caracterizada la siguiente propiedad

\begin{equation*}
\abso{\Gamma_h(\omega)}^{2} = \frac{1}{\pi h} \left( \frac{\text{sen} (h \omega)}{\omega} \right)^{2}
\end{equation*}

Cabe mencionar que puede entenderse al par $g$ y $\Gamma$ como ventanas en el tiempo
y las frecuencias para la serie.

---

Ahora bien, se toma una segunda ventana $W_\tau$ con las siguientes
restricciones para
su funci\'on de respuesta ante frecuencia $w_\tau$

\begin{itemize}
\item $w_{\tau}(t) \geq 0$ para cualesquiera $t$, $\tau$
\item $w_{\tau}(t) \rightarrow 0$ cuando $\lvert t \lvert \rightarrow \infty$, para todo $\tau$
\item $\displaystyle \int_{-\infty}^{\infty} w_{\tau}(t) dt = 1$ para todo $\tau$
\item $\displaystyle \int_{-\infty}^{\infty} \left( w_{\tau}(t) \right)^{2} dt < \infty$ para todo $\tau$
\item Existe una constante $C$ tal que  [T est\'a relacionado con el 'tiempo 0', pero para
tiempos de muestreo grandes se puede reemplazar por $-\infty$ EXCEPTO cerca del inicio y el final dle muestreo]
$$\lim_{\tau\rightarrow\infty} \left[ \tau \int_{t-T}^{t} \lvert W_{\tau}(\lambda) \lvert^{2} d\lambda \right] = C$$
\end{itemize}

%Ahora, si se define 
%$\displaystyle W_{T'}(\lambda) = \int_{-\infty}^{\infty} e^{-i\lambda t}w_{T'}(t) dt $

[posteriormente annadire mas detalles sobre el papel que juega el par $w_\tau$, $W_\tau$]

Como ejemplo, se puede tomar la siguiente funci\'on llamada \textbf{ventana de Daniell}

\begin{equation*}
W_\tau (t) = 
\begin{cases}
{1 \big{/} \tau} & \text{ , } -\nicefrac{1}{2} \tau \leq t \leq \nicefrac{1}{2} \tau
\\
0 & \text{ , otro caso}
\end{cases}
\end{equation*}

La cual se puede demostrar [tengo en algun lado esa demostracion]

$$\lim_{\tau\rightarrow\infty} \left[ \tau \int_{t-T}^{t} \lvert W_{\tau}(\lambda) \lvert^{2} d\lambda \right] = 2\pi$$

-----

Se define el estimador para $f_t$, con $0 \leq t \leq T$
\begin{equation*}
\widehat{f_t}(\omega) = \int_{t-T}^{t} w_{T'}(u) \lvert U(t-u,\omega) \lvert^{2} du
\end{equation*}

Fue demostrado por Priestley (1965, falta citar) que 

[aqui van las expresiones para el valor esperado y la varianza de $\widehat{f_t}$, me falta
escribirlas]

Pero, bajo varios supuesto adicionales [que me falta trascribir] se puede aproximar

\begin{equation*}
E\left[ \widehat{f_t}(\omega) \right] \sim f_t(\omega)
\end{equation*}

\begin{equation*}
\Var{\widehat{f_t}(\omega)} 
\sim 
\frac{C}{\tau} \left(f_t(\omega)\right)^{2} \int_{-\infty}^{\infty} \abso{\Gamma(\theta)}^{4} d\theta
\end{equation*}

Se advierte claramente que $\widehat{f_t}$ es unnestimados aproximadamente insesgado.
Para las ventanas de Bartlett y Daniell usadas como ejemplo, se tiene

\begin{equation*}
\Var{\widehat{f_t}} 
\sim 
\frac{4h}{3\tau} \left(f_t(\omega)\right)^{2}
\end{equation*}

Cabe mencionar que hay una expresi\'on expl\'icita para la covarianza de $\widehat{f_t}$
en para diferentes puntos en el tiempo y las frecuencias. Lamentablemente,
aun me falta escribirlas, son complicadas, y se describen situaciones bajo las
cuales estas covarianzas son negligibles; cabe destacar que TODAS las condiciones 
que se usan para aproximar son b\'asicamente las mismas, y dependen de que la distancia
entre los tiempos y las frecuencias sean tan grandes como sea posible.

------------

El \'ultimo ingrediente del test PSR es una transformaci\'on logar\'itmica
para regular la varianza, y quiza para cortar los bordes de las aproxiamciones.
Se define $Y_{i,j} = \log \left( \widehat{f_{t_i}}(\omega_j) \right)$, con las siguientes propiedades

\begin{equation*}
E\left[ Y_{i,j} \right] \thicksim \log \left( f_{t_i}(\omega_j) \right)
\hspace{4em}
\text{Var}\left( {Y\left(t,\omega\right)}\right) \thicksim \sigma^{2}
\end{equation*}

Luego as\'i, puede escribirse aproximadamente que

$$Y_{i,j} = \log \left( f_{t_i}(\omega_j) \right) + \varepsilon_{i,j}$$

donde $\varepsilon_{i,j}$ va iid tales que

$
E\left[ \varepsilon_{i,j} \right] = 0
\hspace{4em}
\text{Var}\left( \varepsilon_{i,j} \right) \sigma^{2}
$

Priestley cita que con esta informaci\'on incluso se puede considerar que los $\varepsilon_{i,j}$
siguen una distribuci\'on normal cada uno; Nason (2015, falta citar) comenta que
este supuesto no tiene por que cumplirse, y que es una popsible fuente de falsos positivos
para el test. Yo he hecho pruebas de normalidad a los datos, que incluire como anexos
mas tarde.

El test PSR \textit{per se} son tres test ANOVA --en su versi\'on en la que la varianza es conocida--
sobre si los $\varepsilon_{i,j}$ son estad\'isticamente negligibles en total, sobre el tiempo y sobre
las frecuencias. Para el fin de estudiar la estacionariedad, basta con que sean estad\'iticamente
no-negligibles sobre el tiempo.

[Por supuesto que los otros dos test tienen interpretacion: la negigibilidad total da informacion
sobre las marginales, y si estas pueden ser estimadas adecuadamente usando el estimador, si se
combina con negativo para no-estacionariedad es \textbf{efectivamente positivo} para estacionariedad
y toma una forma muy particular (proceso uniformemente modulado). Si sobre las frecuencias resulta
significativo (no-negligible) da informacion sobre la 'aeatoridad total' del proceso.
De tener tiempo, lo incluire como anexo, ya que ninguna de estas caracteristicas es estudiada :( ]

Lo detalles de la implementaci\'on en R estar\'an en la secci\'on de resultados.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%